{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 및 패키지 설치\n",
    "!pip install git+https://github.com/huggingface/transformers.git@main\n",
    "!pip install -q datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoCaptions\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# COCO 데이터셋 다운로드 및 로드\n",
    "coco_dataset = CocoCaptions(root=\"content/drive/MyDrive/abnormal_dataset\", annFile=\"content/drive/MyDrive/abnormal_dataset.json\", transform=None)\n",
    "test_dataset = CocoCaptions(root=\"content/drive/MyDrive/test_abnormal_dataset\", annFile=\"content/drive/MyDrive/test_abnormal_dataset/test_abnormal_dataset.json\", transform=None)\n",
    "\n",
    "# Hugging Face의 transformers 라이브러리에서 AutoProcessor 및 BlipForConditionalGeneration 불러오기\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Processor 및 모델 초기화\n",
    "train_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "valid_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "test_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
