{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cute/.jupyter/abnormal_detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git@main\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision main) to /tmp/pip-req-build-hx6088me\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-hx6088me\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 50290cf7a0234c1b30bfdbf08fbb714fae3a2f19\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cute/venv/lib/python3.12/site-packages (from transformers==4.46.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cute/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cute/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cute/venv/lib/python3.12/site-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cute/venv/lib/python3.12/site-packages (from requests->transformers==4.46.0.dev0) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cute/venv/lib/python3.12/site-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cute/venv/lib/python3.12/site-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n",
      "Requirement already satisfied: nltk in /home/cute/venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/cute/venv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/cute/venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/cute/venv/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/cute/venv/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 및 패키지 설치\n",
    "!pip install git+https://github.com/huggingface/transformers.git@main\n",
    "!pip install -q datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CocoCaptions\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# COCO 데이터셋 다운로드 및 로드\n",
    "coco_dataset = CocoCaptions(root=\"content/drive/MyDrive/abnormal_dataset\", annFile=\"content/drive/MyDrive/abnormal_dataset.json\", transform=None)\n",
    "test_dataset = CocoCaptions(root=\"content/drive/MyDrive/test_abnormal_dataset\", annFile=\"content/drive/MyDrive/test_abnormal_dataset/test_abnormal_dataset.json\", transform=None)\n",
    "\n",
    "# Hugging Face의 transformers 라이브러리에서 AutoProcessor 및 BlipForConditionalGeneration 불러오기\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Processor 및 모델 초기화\n",
    "train_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "valid_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "test_processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cute/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Training Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 9.018897201372608, Validation Loss: 7.220662077990445\n",
      "Training Epoch: 2\n",
      "Epoch 2 - Training Loss: 6.3532005826087845, Validation Loss: 5.5812688610770484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (512). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([3])\n",
      "Generated Caption: a warehouse is on fire.i\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([6])\n",
      "Generated Caption: there were people behind the white car, and there was a big fire behind the people.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([9])\n",
      "Generated Caption: a fire broke out next to a house on the side of a mountain.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([14])\n",
      "Generated Caption: the car is on fire.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([15])\n",
      "Generated Caption: fire is coming down from the mountain behind the truck passing by.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([21])\n",
      "Generated Caption: a fire is burning behind the red fire truck.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([23])\n",
      "Generated Caption: a forest fire is burning behind the yellow car.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([24])\n",
      "Generated Caption: a fire is burning inside the fence and people are surrounding the fence.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([26])\n",
      "Generated Caption: the palm tree is on fire.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([32])\n",
      "Generated Caption: a boat floating on the water is on fire.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([37])\n",
      "Generated Caption: a fire is burning on the side of the building and two firefighters are entering.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([53])\n",
      "Generated Caption: a fire broke out in a high - rise building.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "RuntimeError: output with shape [1, 1, 768] doesn't match the broadcast shape [1, 0, 768] for batch tensor([62])\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([69])\n",
      "Generated Caption: a parked bus is on fire.r\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([83])\n",
      "Generated Caption: there ' s a fire burning in the mountains and a helicopter is spraying water.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([95])\n",
      "Generated Caption: the building is on fire and two people are pouring water on it.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([96])\n",
      "Generated Caption: there is a fire inside the apartment.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([97])\n",
      "Generated Caption: there is a fire on board the ship.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([152])\n",
      "Generated Caption: a car is overturned and on fire next to a white ambulance.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([170])\n",
      "Generated Caption: a house with a brown roof is on fire.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([235])\n",
      "Generated Caption: a boy in gray and a blue bicycle are lying in front of a black car on the road.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([251])\n",
      "Generated Caption: a girl and a bicycle are lying in front of a blue car on the road, and a woman is standing next to them.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([375])\n",
      "Generated Caption: a black car crashed into a telephone pole on the side of the road.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([382])\n",
      "Generated Caption: a white car smashed a telephone pole into the side of the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([383])\n",
      "Generated Caption: a black car and a black motorcycle were smashed in a collision on the road.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([388])\n",
      "Generated Caption: a black car is on fire on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([389])\n",
      "Generated Caption: a light green car is smashed on the road.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([405])\n",
      "Generated Caption: a black car and a gray car collided on the road and were smashed.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([406])\n",
      "Generated Caption: a woman wearing a helmet and a black bicycle are lying in front of a white car on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([411])\n",
      "Generated Caption: a red car and a gray car are colliding on the road.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([412])\n",
      "Generated Caption: a gray car is upside down on the road.e\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([422])\n",
      "Generated Caption: a man in jeans and a blue bicycle are lying in front of a car on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([430])\n",
      "Generated Caption: a gray car is upside down on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([454])\n",
      "Generated Caption: a white car crashed into a tree next to the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([461])\n",
      "Generated Caption: a man in a black helmet and a black motorcycle lying next to a gray car on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([472])\n",
      "Generated Caption: a sky blue car and a blue car collided on the road, and two people are standing next to it.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([495])\n",
      "Generated Caption: a man in pink and a gray car collide on the road, and the man is flying away.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([497])\n",
      "Generated Caption: a man in white and a gray car collide on the road and the man is flying away.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([504])\n",
      "Generated Caption: a gray car is burning on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "RuntimeError: output with shape [1, 1, 768] doesn't match the broadcast shape [1, 0, 768] for batch tensor([522])\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([553])\n",
      "Generated Caption: a man holding the chair is beating a man wearing blue shirt in the hallway.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([570])\n",
      "Generated Caption: two men are punching each other on a crowded street.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([573])\n",
      "Generated Caption: a bald man is kicking a man in gray clothes on the street.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([589])\n",
      "Generated Caption: a man in gray clothes is kicking a man in a suit on the street.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([608])\n",
      "Generated Caption: a woman in shorts is beating a woman in blue with a club in the room.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([623])\n",
      "Generated Caption: a man in gray clothes is kicking a man in red clothes on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([654])\n",
      "Generated Caption: three people are fighting indoors.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([697])\n",
      "Generated Caption: two people are hitting a man in red clothes on the stairs.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([719])\n",
      "Generated Caption: one man in white shirt is attacking a person in blue shirt in the house.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([758])\n",
      "Generated Caption: a man in white shirt is attacking a fallen man in the parking lot.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([815])\n",
      "Generated Caption: a man in gray clothes is kicking a woman wearing mask in the store.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([818])\n",
      "Generated Caption: a woman in white clothes is punching a man in white clothes in the room.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([834])\n",
      "Generated Caption: a woman in black clothes is beating a man in white clothes with club in the store.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([843])\n",
      "Generated Caption: a man wearing blue clothes is punching a woman wearing white clothes in the room.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([866])\n",
      "Generated Caption: a boy in white pants is kicking a girl wearing mask in the store.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([931])\n",
      "Generated Caption: group of people are fighting on the street.r\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([940])\n",
      "Generated Caption: a man wearing hat is strangling a man wearing mask in the subway.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([991])\n",
      "Generated Caption: a boy in white clothes and a boy in green clothes are fighting on the grass.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1010])\n",
      "Generated Caption: a man wearing blue pants is strangling a man in white clothes in front of the store.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1044])\n",
      "Generated Caption: a man wearing red cap is attacking a man in blue clothes on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1050])\n",
      "Generated Caption: two men are on the roof repairing it.i\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1055])\n",
      "Generated Caption: a toddler is climbing out of his bed.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1066])\n",
      "Generated Caption: two women are sitting on the rooftop railing with their arms around their shoulders.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1070])\n",
      "Generated Caption: a woman wearing green pants is sitting on the railing.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1107])\n",
      "Generated Caption: a man in a black jacket is sitting on a high railing.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1141])\n",
      "Generated Caption: toddler wearing pink clothing is climbing out of his bed.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1261])\n",
      "Generated Caption: a man wearing black clothing is sitting on a rooftop railing.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1291])\n",
      "Generated Caption: a man with an umbrella is walking on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1293])\n",
      "Generated Caption: two women and a bald man are walking on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1295])\n",
      "Generated Caption: the girl in red clothes is running on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1299])\n",
      "Generated Caption: a child in green clothes is walking on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1300])\n",
      "Generated Caption: three men are walking on a road with a car.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1303])\n",
      "Generated Caption: two men wearing helmets are standing at the construction site.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1320])\n",
      "Generated Caption: a man in gray clothes is walking on a road with a car.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1326])\n",
      "Generated Caption: a barefoot boy is running on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1333])\n",
      "Generated Caption: a woman wearing sunglasses is standing on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1348])\n",
      "Generated Caption: two men in helmets are walking on the construction site.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1379])\n",
      "Generated Caption: a man with a helmet and a water bottle is standing in front of the construction site.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1465])\n",
      "Generated Caption: a man in red riding a board on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1580])\n",
      "Generated Caption: a man in brown pants is walking on the road.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1597])\n",
      "Generated Caption: a person dressed in blue fell down on the sidewalk next to the grass.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1604])\n",
      "Generated Caption: a man wearing a blue top is lying on the lawn.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1637])\n",
      "Generated Caption: the man wearing jeans is lying on the lawn.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1643])\n",
      "Generated Caption: the person wearing gray clothes is falling next to the flower bed.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1649])\n",
      "Generated Caption: a man wearing brown pants is falling next to a table and chair. a man wearing brown pants is falling next to a table and chair.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1702])\n",
      "Generated Caption: a man in white clothes wearing a hard hat tripped on his bicycle and fell.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1783])\n",
      "Generated Caption: a red - haired woman is lying on the sidewalk.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1796])\n",
      "Generated Caption: a child on crutches fell on a crosswalk.i\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1813])\n",
      "Generated Caption: an elderly woman wearing mint - colored clothes is collapsed under a ladder.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1842])\n",
      "Generated Caption: a man wearing a blue top is falling on the stairs, holding his shoulder.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([1917])\n",
      "Generated Caption: a man wearing a red top is lying in the middle of the room.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2021])\n",
      "Generated Caption: a child wearing red striped clothes fell to the floor.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2055])\n",
      "Generated Caption: an old man in a brown sweater fell down the stairs.i\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2065])\n",
      "Generated Caption: a man wearing a blue t - shirt fell next to a chair in the office.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2093])\n",
      "Generated Caption: a man wearing a white shirt collapsed while sitting on a chair in the office.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2141])\n",
      "Generated Caption: a woman wearing striped short sleeves collapsed in front of her desk in a room.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2149])\n",
      "Generated Caption: a woman wearing a white shirt was lying down with a chair between the desk and the shelf in the room.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2156])\n",
      "Generated Caption: a man wearing a purple top is lying indoors next to a door.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2162])\n",
      "Generated Caption: an old man wearing a white knit is passed out in the room.\n",
      "\n",
      "Batch input_ids shape: torch.Size([1, 512])\n",
      "Batch pixel_values shape: torch.Size([1, 3, 384, 384])\n",
      "Image: tensor([2163])\n",
      "Generated Caption: the blonde old man fell down the stairs.\n",
      "\n",
      "Test complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration, AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "from torchvision.datasets import CocoCaptions\n",
    "\n",
    "# 필요한 라이브러리 설치\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "train_root = \"content/drive/MyDrive/abnormal_dataset\"\n",
    "train_annFile = \"content/drive/MyDrive/abnormal_dataset.json\"\n",
    "test_root = \"content/drive/MyDrive/test_abnormal_dataset\"\n",
    "test_annFile = \"content/drive/MyDrive/test_abnormal_dataset/test_abnormal_dataset.json\"\n",
    "\n",
    "# COCO 데이터셋 로드\n",
    "coco_train_dataset = CocoCaptions(root=train_root, annFile=train_annFile, transform=None)\n",
    "coco_test_dataset = CocoCaptions(root=test_root, annFile=test_annFile, transform=None)\n",
    "\n",
    "# Processor 및 모델 초기화\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# 이미지 캡션 데이터셋 클래스 정의\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, return_image_name=False):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.return_image_name = return_image_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, captions = self.dataset[idx]\n",
    "        text = captions[0]\n",
    "        encoding = self.processor(images=image, text=text, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "\n",
    "        if self.return_image_name:\n",
    "            image_name = self.dataset.ids[idx]  # 이미지 파일 이름 반환\n",
    "            return encoding, image_name\n",
    "        else:\n",
    "            return encoding\n",
    "\n",
    "# 데이터셋 분할\n",
    "total_size = len(coco_train_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "# 학습, 검증, 테스트 데이터셋 생성\n",
    "train_dataset, valid_dataset = random_split(coco_train_dataset, [train_size, valid_size])\n",
    "train_dataset = ImageCaptioningDataset(train_dataset, processor)\n",
    "valid_dataset = ImageCaptioningDataset(valid_dataset, processor)\n",
    "test_dataset = ImageCaptioningDataset(coco_test_dataset, processor, return_image_name=True)\n",
    "\n",
    "# 데이터로더 초기화\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=4)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "# AdamW 옵티마이저 및 장치 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(2):\n",
    "    print(f\"Training Epoch: {epoch + 1}\")\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        torch.cuda.empty_cache()\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0.0\n",
    "        for val_batch in valid_dataloader:\n",
    "            val_input_ids = val_batch.pop(\"input_ids\").to(device)\n",
    "            val_pixel_values = val_batch.pop(\"pixel_values\").to(device)\n",
    "            val_outputs = model(input_ids=val_input_ids, pixel_values=val_pixel_values, labels=val_input_ids)\n",
    "            total_val_loss += val_outputs.loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_dataloader)}, Validation Loss: {total_val_loss / len(valid_dataloader)}\")\n",
    "\n",
    "# Test loop\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "with torch.no_grad():\n",
    "    # 이미지와 생성된 캡션을 저장할 파일 열기\n",
    "    with open(\"content/drive/MyDrive/blip_finetuning_weight/test_predictions.txt\", \"w\") as predictions_file:\n",
    "        for test_batch, image_name in test_dataloader:\n",
    "            # 각 배치의 크기를 확인하여 디버깅\n",
    "            print(f\"Batch input_ids shape: {test_batch['input_ids'].shape}\")\n",
    "            print(f\"Batch pixel_values shape: {test_batch['pixel_values'].shape}\")\n",
    "\n",
    "            input_ids = test_batch.pop(\"input_ids\").to(device)\n",
    "            pixel_values = test_batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "            # 문제 확인: input_ids가 비정상적으로 설정된 경우 디버그 메시지 출력\n",
    "            if input_ids.shape[1] == 0:\n",
    "                print(f\"Skipping batch {image_name} due to empty input_ids.\")\n",
    "                continue\n",
    "\n",
    "            # generate() 호출 시 max_new_tokens 설정\n",
    "            try:\n",
    "                outputs = model.generate(input_ids=input_ids, pixel_values=pixel_values, max_new_tokens=50)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError: {e} for batch {image_name}\")\n",
    "                continue\n",
    "\n",
    "            # 생성된 캡션 디코딩\n",
    "            generated_captions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            # 이미지 이름과 생성된 캡션을 파일에 저장하고 출력\n",
    "            for caption in generated_captions:\n",
    "                predictions_file.write(f\"Image: {image_name}\\nGenerated Caption: {caption}\\n\\n\")\n",
    "                print(f\"Image: {image_name}\\nGenerated Caption: {caption}\\n\")\n",
    "\n",
    "print(\"Test complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cute/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Training Epoch: 1\n",
      "Epoch 1 - Training Loss: 9.04211517338339, Validation Loss: 7.257724892009389\n",
      "Image: ('fire00000004.png',)\n",
      "Generated Caption: a warehouse is on fire.er\n",
      "\n",
      "Image: ('fire00000007.png',)\n",
      "Generated Caption: there were people behind the white car, and there was a big fire behind the people.e\n",
      "\n",
      "Image: ('fire00000010.png',)\n",
      "Generated Caption: a fire broke out next to a house on the side of a mountain.r\n",
      "\n",
      "Image: ('fire00000015.png',)\n",
      "Generated Caption: the car is on fire.\n",
      "\n",
      "Image: ('fire00000016.png',)\n",
      "Generated Caption: fire is coming down from the mountain behind the truck passing by.er\n",
      "\n",
      "Image: ('fire00000022.png',)\n",
      "Generated Caption: a fire is burning behind the red fire truck.e\n",
      "\n",
      "Image: ('fire00000024.png',)\n",
      "Generated Caption: a forest fire is burning behind the yellow car.e\n",
      "\n",
      "Image: ('fire00000025.png',)\n",
      "Generated Caption: a fire is burning inside the fence and people are surrounding the fence.\n",
      "\n",
      "Image: ('fire00000027.png',)\n",
      "Generated Caption: the palm tree is on fire.\n",
      "\n",
      "Image: ('fire00000033.png',)\n",
      "Generated Caption: a boat floating on the water is on fire.r\n",
      "\n",
      "Image: ('fire00000038.png',)\n",
      "Generated Caption: a fire is burning on the side of the building and two firefighters are entering.\n",
      "\n",
      "Image: ('fire00000054.png',)\n",
      "Generated Caption: a fire broke out in a high - rise building.\n",
      "\n",
      "Image: ('fire00000063.png',)\n",
      "Generated Caption: the house is engulfed in fire, and firefighters are extinguishing the fire on the left.er\n",
      "\n",
      "Image: ('fire00000070.png',)\n",
      "Generated Caption: a parked bus is on fire.r\n",
      "\n",
      "Image: ('fire00000084.png',)\n",
      "Generated Caption: there ' s a fire burning in the mountains and a helicopter is spraying water.\n",
      "\n",
      "Image: ('fire00000096.png',)\n",
      "Generated Caption: the building is on fire and two people are pouring water on it.\n",
      "\n",
      "Image: ('fire00000097.png',)\n",
      "Generated Caption: there is a fire inside the apartment.\n",
      "\n",
      "Image: ('fire00000098.png',)\n",
      "Generated Caption: there is a fire on board the ship.er\n",
      "\n",
      "Image: ('fire00000153.png',)\n",
      "Generated Caption: a car is overturned and on fire next to a white ambulance.\n",
      "\n",
      "Image: ('fire00000171.png',)\n",
      "Generated Caption: a house with a brown roof is on fire.r\n",
      "\n",
      "Image: ('traffic_accident00000257.jpg',)\n",
      "Generated Caption: a boy in gray and a blue bicycle are lying in front of a black car on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000274.jpg',)\n",
      "Generated Caption: a girl and a bicycle are lying in front of a blue car on the road, and a woman is standing next to them.\n",
      "\n",
      "Image: ('traffic_accident00000032.jpg',)\n",
      "Generated Caption: a black car crashed into a telephone pole on the side of the road.r\n",
      "\n",
      "Image: ('traffic_accident00000039.jpg',)\n",
      "Generated Caption: a white car smashed a telephone pole into the side of the road.e\n",
      "\n",
      "Image: ('traffic_accident00000040.jpg',)\n",
      "Generated Caption: a black car and a black motorcycle were smashed in a collision on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000045.jpg',)\n",
      "Generated Caption: a black car is on fire on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000046.jpg',)\n",
      "Generated Caption: a light green car is smashed on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000063.jpg',)\n",
      "Generated Caption: a black car and a gray car collided on the road and were smashed.e\n",
      "\n",
      "Image: ('traffic_accident00000064.jpg',)\n",
      "Generated Caption: a woman wearing a helmet and a black bicycle are lying in front of a white car on the road.er\n",
      "\n",
      "Image: ('traffic_accident00000070.jpg',)\n",
      "Generated Caption: a red car and a gray car are colliding on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000071.jpg',)\n",
      "Generated Caption: a gray car is upside down on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000084.jpg',)\n",
      "Generated Caption: a man in jeans and a blue bicycle are lying in front of a car on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000092.jpg',)\n",
      "Generated Caption: a gray car is upside down on the road.r\n",
      "\n",
      "Image: ('traffic_accident00000124.jpg',)\n",
      "Generated Caption: a white car crashed into a tree next to the road.r\n",
      "\n",
      "Image: ('traffic_accident00000131.jpg',)\n",
      "Generated Caption: a man in a black helmet and a black motorcycle lying next to a gray car on the road.e\n",
      "\n",
      "Image: ('traffic_accident00000142.jpg',)\n",
      "Generated Caption: a sky blue car and a blue car collided on the road, and two people are standing next to it.e\n",
      "\n",
      "Image: ('traffic_accident00000169.jpg',)\n",
      "Generated Caption: a man in pink and a gray car collide on the road, and the man is flying away.er\n",
      "\n",
      "Image: ('traffic_accident00000171.jpg',)\n",
      "Generated Caption: a man in white and a gray car collide on the road and the man is flying away.\n",
      "\n",
      "Image: ('traffic_accident00000178.jpg',)\n",
      "Generated Caption: a gray car is burning on the road.r\n",
      "\n",
      "Image: ('traffic_accident00000204.jpg',)\n",
      "Generated Caption: three cars are colliding on the road.r\n",
      "\n",
      "Image: ('fight00000012.jpg',)\n",
      "Generated Caption: a man holding the chair is beating a man wearing blue shirt in the hallway.\n",
      "\n",
      "Image: ('fight00000031.jpg',)\n",
      "Generated Caption: two men are punching each other on a crowded street.\n",
      "\n",
      "Image: ('fight00000034.jpg',)\n",
      "Generated Caption: a bald man is kicking a man in gray clothes on the street.\n",
      "\n",
      "Image: ('fight00000051.jpg',)\n",
      "Generated Caption: a man in gray clothes is kicking a man in a suit on the street.\n",
      "\n",
      "Image: ('fight00000073.png',)\n",
      "Generated Caption: a woman in shorts is beating a woman in blue with a club in the room.\n",
      "\n",
      "Image: ('fight00000093.jpg',)\n",
      "Generated Caption: a man in gray clothes is kicking a man in red clothes on the road.\n",
      "\n",
      "Image: ('fight00000128.jpg',)\n",
      "Generated Caption: three people are fighting indoors.\n",
      "\n",
      "Image: ('fight00000182.jpg',)\n",
      "Generated Caption: two people are hitting a man in red clothes on the stairs.r\n",
      "\n",
      "Image: ('fight00000205.png',)\n",
      "Generated Caption: one man in white shirt is attacking a person in blue shirt in the house.r\n",
      "\n",
      "Image: ('fight00000252.png',)\n",
      "Generated Caption: a man in white shirt is attacking a fallen man in the parking lot.r\n",
      "\n",
      "Image: ('fight00000347.png',)\n",
      "Generated Caption: a man in gray clothes is kicking a woman wearing mask in the store.\n",
      "\n",
      "Image: ('fight00000362.png',)\n",
      "Generated Caption: a woman in white clothes is punching a man in white clothes in the room.\n",
      "\n",
      "Image: ('fight00000402.png',)\n",
      "Generated Caption: a woman in black clothes is beating a man in white clothes with club in the store.\n",
      "\n",
      "Image: ('fight00000432.png',)\n",
      "Generated Caption: a man wearing blue clothes is punching a woman wearing white clothes in the room.\n",
      "\n",
      "Image: ('fight00000496.png',)\n",
      "Generated Caption: a boy in white pants is kicking a girl wearing mask in the store.\n",
      "\n",
      "Image: ('fight00000610.png',)\n",
      "Generated Caption: group of people are fighting on the street.r\n",
      "\n",
      "Image: ('fight00000622.png',)\n",
      "Generated Caption: a man wearing hat is strangling a man wearing mask in the subway.\n",
      "\n",
      "Image: ('fight00000699.jpg',)\n",
      "Generated Caption: a boy in white clothes and a boy in green clothes are fighting on the grass.\n",
      "\n",
      "Image: ('fight00000727.png',)\n",
      "Generated Caption: a man wearing blue pants is strangling a man in white clothes in front of the store.er\n",
      "\n",
      "Image: ('fight00000772.png',)\n",
      "Generated Caption: a man wearing red cap is attacking a man in blue clothes on the road.r\n",
      "\n",
      "Image: ('caution000003.jpg',)\n",
      "Generated Caption: two men are on the roof repairing it.\n",
      "\n",
      "Image: ('caution000008.jpeg',)\n",
      "Generated Caption: a toddler is climbing out of his bed.er\n",
      "\n",
      "Image: ('caution000019.jpg',)\n",
      "Generated Caption: two women are sitting on the rooftop railing with their arms around their shoulders.r\n",
      "\n",
      "Image: ('caution000023.jpg',)\n",
      "Generated Caption: a woman wearing green pants is sitting on the railing.\n",
      "\n",
      "Image: ('caution000060.jpeg',)\n",
      "Generated Caption: a man in a black jacket is sitting on a high railing.\n",
      "\n",
      "Image: ('caution000094.jpg',)\n",
      "Generated Caption: toddler wearing pink clothing is climbing out of his bed.er\n",
      "\n",
      "Image: ('caution000215.jpeg',)\n",
      "Generated Caption: a man wearing black clothing is sitting on a rooftop railing.r\n",
      "\n",
      "Image: ('caution00000001.jpg',)\n",
      "Generated Caption: a man with an umbrella is walking on the road.\n",
      "\n",
      "Image: ('caution00000003.jpg',)\n",
      "Generated Caption: two women and a bald man are walking on the road.\n",
      "\n",
      "Image: ('caution00000005.jpg',)\n",
      "Generated Caption: the girl in red clothes is running on the road.\n",
      "\n",
      "Image: ('caution00000009.jpg',)\n",
      "Generated Caption: a child in green clothes is walking on the road.\n",
      "\n",
      "Image: ('caution00000011.jpg',)\n",
      "Generated Caption: three men are walking on a road with a car.\n",
      "\n",
      "Image: ('caution00000014.jpg',)\n",
      "Generated Caption: two men wearing helmets are standing at the construction site.\n",
      "\n",
      "Image: ('caution00000033.jpg',)\n",
      "Generated Caption: a man in gray clothes is walking on a road with a car.\n",
      "\n",
      "Image: ('caution00000039.jpg',)\n",
      "Generated Caption: a barefoot boy is running on the road.\n",
      "\n",
      "Image: ('caution00000046.jpg',)\n",
      "Generated Caption: a woman wearing sunglasses is standing on the road.\n",
      "\n",
      "Image: ('caution00000061.jpg',)\n",
      "Generated Caption: two men in helmets are walking on the construction site.\n",
      "\n",
      "Image: ('caution00000093.jpg',)\n",
      "Generated Caption: a man with a helmet and a water bottle is standing in front of the construction site.er\n",
      "\n",
      "Image: ('caution00000190.jpg',)\n",
      "Generated Caption: a man in red riding a board on the road.\n",
      "\n",
      "Image: ('caution00000320.jpg',)\n",
      "Generated Caption: a man in brown pants is walking on the road.\n",
      "\n",
      "Image: ('fall00000009.png',)\n",
      "Generated Caption: a person dressed in blue fell down on the sidewalk next to the grass.r\n",
      "\n",
      "Image: ('fall00000018.png',)\n",
      "Generated Caption: a man wearing a blue top is lying on the lawn.\n",
      "\n",
      "Image: ('fall00000058.png',)\n",
      "Generated Caption: the man wearing jeans is lying on the lawn.e\n",
      "\n",
      "Image: ('fall00000068.png',)\n",
      "Generated Caption: the person wearing gray clothes is falling next to the flower bed.\n",
      "\n",
      "Image: ('fall00000074.png',)\n",
      "Generated Caption: a man wearing brown pants is falling next to a table and chair. a man wearing brown pants is falling next to a table and chair.\n",
      "\n",
      "Image: ('fall00000141.png',)\n",
      "Generated Caption: a man in white clothes wearing a hard hat tripped on his bicycle and fell.\n",
      "\n",
      "Image: ('fall00000227.png',)\n",
      "Generated Caption: a red - haired woman is lying on the sidewalk.\n",
      "\n",
      "Image: ('fall00000240.png',)\n",
      "Generated Caption: a child on crutches fell on a crosswalk.r\n",
      "\n",
      "Image: ('fall00000258.png',)\n",
      "Generated Caption: an elderly woman wearing mint - colored clothes is collapsed under a ladder.r\n",
      "\n",
      "Image: ('fall00000290.jpeg',)\n",
      "Generated Caption: a man wearing a blue top is falling on the stairs, holding his shoulder.er\n",
      "\n",
      "Image: ('fall00000370.png',)\n",
      "Generated Caption: a man wearing a red top is lying in the middle of the room.r\n",
      "\n",
      "Image: ('fall00000474.jpg',)\n",
      "Generated Caption: a child wearing red striped clothes fell to the floor.\n",
      "\n",
      "Image: ('fall00000508.jpg',)\n",
      "Generated Caption: an old man in a brown sweater fell down the stairs.e\n",
      "\n",
      "Image: ('fall00000518.jpg',)\n",
      "Generated Caption: a man wearing a blue t - shirt fell next to a chair in the office.r\n",
      "\n",
      "Image: ('fall00000546.jpg',)\n",
      "Generated Caption: a man wearing a white shirt collapsed while sitting on a chair in the office.r\n",
      "\n",
      "Image: ('fall00000594.jpg',)\n",
      "Generated Caption: a woman wearing striped short sleeves collapsed in front of her desk in a room.er\n",
      "\n",
      "Image: ('fall00000602.jpg',)\n",
      "Generated Caption: a woman wearing a white shirt was lying down with a chair between the desk and the shelf in the room.\n",
      "\n",
      "Image: ('fall00000609.jpg',)\n",
      "Generated Caption: a man wearing a purple top is lying indoors next to a door.\n",
      "\n",
      "Image: ('fall00000615.jpg',)\n",
      "Generated Caption: an old man wearing a white knit is passed out in the room.r\n",
      "\n",
      "Image: ('fall00000616.jpg',)\n",
      "Generated Caption: the blonde old man fell down the stairs.r\n",
      "\n",
      "Test complete.\n"
     ]
    }
   ],
   "source": [
    "# 최최최최종 !!!!!!!! \n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration, AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "from torchvision.datasets import CocoCaptions\n",
    "\n",
    "# 필요한 라이브러리 설치\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "train_root = \"content/drive/MyDrive/abnormal_dataset\"\n",
    "train_annFile = \"content/drive/MyDrive/abnormal_dataset.json\"\n",
    "test_root = \"content/drive/MyDrive/test_abnormal_dataset\"\n",
    "test_annFile = \"content/drive/MyDrive/test_abnormal_dataset/test_abnormal_dataset.json\"\n",
    "\n",
    "# COCO 데이터셋 로드\n",
    "coco_train_dataset = CocoCaptions(root=train_root, annFile=train_annFile, transform=None)\n",
    "coco_test_dataset = CocoCaptions(root=test_root, annFile=test_annFile, transform=None)\n",
    "\n",
    "# Processor 및 모델 초기화\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# 이미지 캡션 데이터셋 클래스 정의\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, return_image_name=False):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.return_image_name = return_image_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, captions = self.dataset[idx]\n",
    "        text = captions[0]\n",
    "\n",
    "        # COCO 데이터셋에서 실제 이미지 파일 이름을 가져옴\n",
    "        if hasattr(self.dataset, 'ids'):\n",
    "            # 파일 경로를 추출하고 파일 이름만 반환\n",
    "            image_name = self.dataset.ids[idx]\n",
    "            image_file_name = os.path.basename(self.dataset.coco.imgs[image_name]['file_name'])  # 파일 이름 추출\n",
    "        else:\n",
    "            image_file_name = f\"image_{idx}.jpg\"\n",
    "\n",
    "        # 입력 인코딩 생성\n",
    "        encoding = self.processor(images=image, text=text, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "\n",
    "        if self.return_image_name:\n",
    "            return encoding, image_file_name\n",
    "        else:\n",
    "            return encoding\n",
    "\n",
    "# 데이터셋 분할\n",
    "total_size = len(coco_train_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "# 학습, 검증, 테스트 데이터셋 생성\n",
    "train_dataset, valid_dataset = random_split(coco_train_dataset, [train_size, valid_size])\n",
    "train_dataset = ImageCaptioningDataset(train_dataset, processor)\n",
    "valid_dataset = ImageCaptioningDataset(valid_dataset, processor)\n",
    "test_dataset = ImageCaptioningDataset(coco_test_dataset, processor, return_image_name=True)\n",
    "\n",
    "# 데이터로더 초기화\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=4)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "# AdamW 옵티마이저 및 장치 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(1):\n",
    "    print(f\"Training Epoch: {epoch + 1}\")\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        torch.cuda.empty_cache()\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0.0\n",
    "        for val_batch in valid_dataloader:\n",
    "            val_input_ids = val_batch.pop(\"input_ids\").to(device)\n",
    "            val_pixel_values = val_batch.pop(\"pixel_values\").to(device)\n",
    "            val_outputs = model(input_ids=val_input_ids, pixel_values=val_pixel_values, labels=val_input_ids)\n",
    "            total_val_loss += val_outputs.loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_dataloader)}, Validation Loss: {total_val_loss / len(valid_dataloader)}\")\n",
    "\n",
    "# Test loop\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "with torch.no_grad():\n",
    "    # 이미지와 생성된 캡션을 저장할 파일 열기\n",
    "    with open(\"content/drive/MyDrive/blip_finetuning_weight/test_predictions.txt\", \"w\") as predictions_file:\n",
    "        for test_batch, image_name in test_dataloader:\n",
    "            input_ids = test_batch.pop(\"input_ids\").to(device)\n",
    "            pixel_values = test_batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "            # generate() 호출 시 max_new_tokens 설정\n",
    "            outputs = model.generate(input_ids=input_ids, pixel_values=pixel_values, max_new_tokens=50)\n",
    "\n",
    "            # 생성된 캡션 디코딩\n",
    "            generated_captions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            # 이미지 이름과 생성된 캡션을 파일에 저장하고 출력\n",
    "            for caption in generated_captions:\n",
    "                predictions_file.write(f\"Image: {image_name}\\nGenerated Caption: {caption}\\n\\n\")\n",
    "                print(f\"Image: {image_name}\\nGenerated Caption: {caption}\\n\")\n",
    "\n",
    "print(\"Test complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
